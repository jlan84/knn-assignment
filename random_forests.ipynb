{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Random Forests\n",
      "=========================\n",
      "***\n",
      "\n",
      "### How do I handle data when the number of variables is very high? "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "Often a problem that needs to be tackled is so large or complex that we need a group of experts not just a single one to tackle it.  Linux, for example, is such a complex system that building it took hundreds of experts.\n",
      "\n",
      "What if we could harness the decision-making power and the subject matter expertise of many experts and use it in Data Science?  There is such a technique called Random Forests which uses collective decision-making to improve on the outcome possible with a single decision maker.  In this approach each software \"expert\" uses a tree-based algorithm to do their bit and then a collection of such trees is used to compute or evolve a model that is better than the output of any one expert."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Decision Trees from data.\n",
      "\n",
      "First let's understand how the individual tree-based decision making works.\n",
      "\n",
      "Consider a pool of college applicants applying to Super Exclusive Institute Of Tech.  The average SAT score for admission has historically been 2,200.  And the average GPA 4.9.  We are given the application info on 1,000 applicants.  We are asked to create a model that will allow us to predict students most likely to be admitted.  How do we go about doing this?\n",
      "\n",
      "One approach would be to first divide the applicants into those that have SAT score over 2,200 and then call this the \"more likely\" group.  Then to further test for the GPA in this group and split it into two based on GPA less than or equal to 4.9 vs GPA over 4.9.  We call the former subgroup \"most likely\" and the latter a \"high maybe\".\n",
      "\n",
      "Then we do the same thing to the group with SAT score below 2,200 calling the high GPA subgroup a \"maybe\", and the low GPA subgroup a \"probably not\".  This seems reasonable but there are a number of questions that arise.\n",
      "\n",
      "* What if we split on GPA first and then SAT scores - would we get the same groupings?  i.e. what is the best way to split?\n",
      "* What if we used more criteria such as essay evaluation scores, extra curriculars, awards and distinctions in sports etc. i.e. how many attributes should we use to create splits and what are the most significant attributes.\n",
      "* We were given averages, but what about the spread, what about outliers? i.e. how does the distribution of attributes affect misclassification?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "A decision tree uses the intrinsic structure of the data to make these splits.  \n",
      "\n",
      "![titanic](http://upload.wikimedia.org/wikipedia/commons/f/f3/CART_tree_titanic_survivors.png)\n",
      "\n",
      "This graphic from Wikipedia [1] represents intrinsic structure of the \"Titanic\" dataset, data on the survivors of the Titanic disater.\n",
      "\n",
      "The information in a decision tree format. The numbers next to each node are the probability of survival and the % of the observations that were assigned to (classified as) the category represented by this node. Each left branch corresponds to a \"yes\" answer, the right one a \"no\".  Each green node represents \"survived\", each red one \"did not survive\".\n",
      "\n",
      "The number of spouses or siblings aboard is recorded as \"sibsp\".\n",
      "\n",
      "As is well known you had a much smaller chance of surviving if you were male and in the less expensive berths. You had a much greater chance of surviving if you were an infant or female and in the most expensive berths."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Measures of \"goodness\" for decision trees\n",
      "The science of Decision Trees quantifies all this using measures called Information Gain and Entropy.  Essentially we want to have just the right amount of splits so that we don't keep splitting a group once we have the \"best\" split.  So how do we know when one way to split is better than another.  Obviously if one split leads to a clean partition into 'admit' vs. 'reject' then it's good.  But what if we split on say essay scores right at the top.  We might get groups that have wide variation in GPA, STA in both halves. So we really haven't improved our ability to predict much because both groups seem equally mixed.\n",
      "\n",
      "This kind of variation in a set indicates a higher \"entropy\" while a set with all identical members has very low or zero \"entropy\". So, when we split a set we want the halves to be more distinct from each other and the members in the group to be more like each other -- i.e. we want entropy to go down as we keep splitting.  So if we use an approach that doesn't reduce the entropy by much it is probably not a good attribute or a good value to split on.  \n",
      "\n",
      "If we take a Decision Tree that has been created and we reverse the process, then when we combine two nodes, we will increase entropy or variation as groups get combined,  The gain in entropy is called Information Gain.  So the best splits are those which give the best Information Gain when reversed.\n",
      "\n",
      "This is all very loose but has a strong mathematical foundation that is used to construct the  modeling software that creates such \"decision tree\" models.\n",
      "\n",
      "When given a set of samples with many attributes, a decision tree model will identify the attributes that are best to split on and the values of those attributes that we should use to do the splitting.  It will then print out a number of parameters, including number of attributes used to split, which ones, and Information Gain,... etc.\n",
      "\n",
      "So how does modeling software decide the best tree?  It tries every one and compares Information Gain for each and then picks the best one."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Now for the Forest\n",
      "Decision Trees present a simple clean conceptual model to understand classification by an iterative procedure. However, in practice, a single Decision Tree is not very useful for real world problems involving a large number of variables and moderate to large sized data.  \n",
      "For this we need heavy artillery. A group of \"experts\" constituting a Random Forest.  \n",
      "\n",
      "But what is an \"expert\" in this scenario?\n",
      "\n",
      "If we consider our \"expert\" to have in their head a decision tree modeler and we assemble say 100 such experts, then, loosely speaking, we have the makings of a Random Forest. We want a collection of experts to decide our result, expecting that the result will be much better than a single one.  So we will need some way to decide how to collate and sort through the \"opinions\".  \n",
      "\n",
      "If you recall the Olympic Gymnastic competitions or diving competitions where a panel of judges scores a participant, you might remember that the top and bottom scores are dropped and the rest are averaged.  A Random Forest algorithm uses such techniques to eliminate some of the opinions but might randomly drop some percentage and then rerun the \"competition\", doing this each time and then averaging the result after say 100 such trials."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Why use such complicated techniques?\n",
      "Well for one, they are more accurate, as mathematicaly provable.  But also because, when we have 10s or 100s of attributes Random Forests are able to surface the most significant ones and use these in their modeling without any extra effort on our part.  So what's the catch?  This comes at some computational cost so our model may run for many minutes instead of a few seconds even with a few thousand samples, since orders of magnitude more calculations are being done.  However there are many more benefits for this one cost.  \n",
      "Random Forests are much more tolerant of missing values, bad data, and outliers, and can handle mixed data types, numerical and categorical.\n",
      "\n",
      "We will explore a rich data set generated from the accelerometer and gyroscope of mobile phones, and use it to understand various activities of the user - such as sitting, standing, walking etc., based on particular combinations of the data attributes.  Our data has more than 500 such attributes and the data is also messy and rich so this is a good candiadte for combining domain knowledge with the power of Random Forests in the exploration and analysis to follow."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Introducing Forests\n",
      "\n",
      "Now we're going to use a large and messy data set from a familiar source object and then prepare it for analysis using Random Forests.\n",
      "\n",
      "Why do we want to use Random Forests? This will become clear very shortly.\n",
      "\n",
      "We will use a data set of mobile phone accelerometer and gyroscope readings to create a predictive model. The data set is found in raw form at the UCI Repository.  The data set readings encode data on mobile phone orientation and motion of the wearer of the phone.\n",
      "\n",
      "The subject is known to be doing one of six activities - sitting, standing, lying down, walking, walking up, and walking down.\n",
      "\n",
      "##Methods\n",
      "\n",
      "Our goal is to predict, given one data point, which activity they are doing.\n",
      "We set ourself a goal of creating a model with understandable variables rather than a black box model. We have the choice of creating a black box model that just has variables and coefficients.  When given a data point we feed it to the model and out pops an answer.  This generally works but is simply too much \"magic\" to give us any help in building our intuition or giving us any opportunity to use our domain knowledge.\n",
      "\n",
      "So we are going to open the box a bit and we are going to use domain knowledge combined with the massive power of Random Forests once we have some intuition going.  We find that in the long run this is a much more satisfying approach and also, it appears, a much more powerful one.\n",
      "\n",
      "We will reduce the independent variable set to 36 variables using domain knowledge alone and then use Random Forests to predict the variable \u2018activity\u2019. \n",
      "This may not be the best model from the point of view of accuracy, but we want to understand what is going on and from that perspective it turns out to be much better.\n",
      "\n",
      "We use accuracy measures Positive and Negative Prediction Value, Sensitivity and Specificity to rate our model."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Data Description\n",
      "\n",
      "* The given data set contains activity data for 21 subjects. \n",
      "* The data set has 7,352 rows with 561 numeric data columns plus 2 columns \u2018subject\u2019, an integer, and \u2018activity\u2019, a character string. \n",
      "* Since we have 563 total columns we will dispense with the step of creating a formal data dictionary and refer to feature_info.txt instead\n",
      "\n",
      "* Initial exploration of the data shows it has dirty column name text with a number of problems:\n",
      "    * Duplicate column names - multiple occurrences. \n",
      "    * Inclusion of (  ) in column names.\n",
      "    * Extra ) in some column names.\n",
      "    * Inclusion of \u2018-\u2019 in column names.\n",
      "    * Inclusion of multiple \u2018,\u2019 in column names\n",
      "    * Many column names contain \u201cBodyBody\u201d which we assume is a typo."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Exercise: \n",
      "\n",
      "* Read the dataset into pandas (samsungdata.csv)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We want to create an interpretable model rather than use Random Forests as a black box. So we will need to understand our variables and leverage our intuition about them.\n",
      "\n",
      "To plan the data exploration, the documentation of the data set from the [UCI website](http://archive.ics.uci.edu/ml/datasets/Human+Activity+Recognition+Using+Smartphones) is very useful and we study it in detail. \n",
      "Especially the file feature_info.txt is very important in understanding our variables. It is, in effect, the data dictionary which we have avoided listing here.\n",
      "Also the explanation for terminology which we use is in feature_info.txt. So going through it in some detail is critical."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Exercise\n",
      "\n",
      "* Change \u2018activity\u2019 to be a categorical variable\n",
      "* Keep \u2018subject\u2019 as integer \n",
      "* Identify and remove duplicate column names - multiple occurrences. \n",
      "* Identify and fix inclusion of (  ) in column names. How will you fix this?\n",
      "* Identify and fix extra ) in some column names. How will you fix this?\n",
      "* Identify and fixInclusion of \u2018-\u2019 in column names. How will you fix this?\n",
      "* Identify and fixInclusion of multiple \u2018,\u2019 in column names. How will you fix this?\n",
      "* Identify and fix column names containing \u201cBodyBody\u201d."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "--------\n",
      "\n",
      "### Notes \n",
      "\n",
      "* You can just use blind brute force techniques and get useful results OR  \n",
      "* You can short circuit a lot of that and use domain knowledge.  This data set highlights the power you get from domain knowledge.   \n",
      "* It also nudges us out of our comfort zone to seek supporting knowledge from semanticaly adjacent data sources to empower the analysis further.  \n",
      "* This underlines the fact, obvious in restrospect, that you never get the data and all supporting information in a neat bundle.    \n",
      "* You have to clean it up - we learnt that earlier, but we also may have to be willing to expand our knowledge, do a little research to enhance our background expertise.  \n",
      "\n",
      "\n",
      "So this particular data set may seem a little techy but it could easily be in the direction of bio, or finance or mechanics of fractures or sports analytics or whatever - a data scientist should be willing to get hands *and* mind dirty.  The most successful ones are/will be the ones that are willing to be interdisciplinary.     \n",
      "\n",
      "__That's__ the implicit lesson here. \n",
      "\n",
      "----------"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Aside from understanding what each variable represents, we also want to get some technical background about the meaning of each variable.\n",
      "\n",
      "So we use the [Android Developer Reference](http://developer.android.com/guide/topics/sensors/sensors_overview.html#sensors-coords) to educate ourselves about each of the physical parameters that are important.\n",
      "\n",
      "In this way we extend our domain knowledge so that we understand the language of the data - we allow it to come alive and figuratively speak to us and reveal it's secrets.  The more we learn about the background context from which the data comes, the better, faster, and deeper our exploration of the data will be.\n",
      "\n",
      "In this case see that the variables have X, Y, Z prefixes/suffixes and the Android Developer Reference gives us the specific reference frame with which these are measured. They are vector components of jerk and acceleration, the angles are measured with respect to the direction of gravity or more precisely the vector acceleration due to gravity.\n",
      "\n",
      "We use this information and combine it with some intuition about motion, velocity, acceleration etc."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Variable Reduction\n",
      "\n",
      "So we dig into the variables and make some quick notes.\n",
      "\n",
      "Before we go further, you'll need to open a file in the dataset directory for the HAR data set.\n",
      "There is a file called feature_info.txt.  This file describes each feature, it's physical significance and also describes features that are derived from raw data by doing some averaging, or sampling or some operation that gives a numerical results.\n",
      "\n",
      "We want to look at \n",
      "\n",
      "a) all the variable names  \n",
      "b) physical quantities\n",
      "\n",
      "and take some time to understand these."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Once we spend some time doing all that, we can extract some useful guidelines using physical understanding and common sense.\n",
      "\n",
      "* In static activities (sit, stand, lie down) motion information will not be very useful.\n",
      "* In the dynamic activities (3 types of walking) motion will be significant.\n",
      "* Angle variables will be useful both in differentiating \u201clie vs stand\u201d and \u201cwalk up vs walk down\u201d.\n",
      "* Acceleration and Jerk variables are important in distinguishing various kinds of motion while filtering out random tremors while static.\n",
      "* Mag and angle variables contain the same info as (= strongly correlated with) XYZ variables \n",
      "* We choose to focus on the latter as they are simpler to reason about. \n",
      "* This is a very important point to understand as it results in elimination of a few hundred variables.\n",
      "* We ignore the *band* variables as we have no simple way to interpret the meaning and relate them to physical activities.\n",
      "* -mean and -std are important, -skewness and -kurtosis may also be hence we include all these.\n",
      "* We see the usefulness of some of these variables as predictors in Figure 1. \n",
      "which shows some of our exploration and validates our thinking."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Exercise\n",
      "\n",
      "* Make a histogram of Body Acceleration Magnitude to evaluate that variable as a predictor of static vs dynamic activities. This is an example of data exploration in support of our heuristic variable selection using domain knowledge.\n",
      "\n",
      "---"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Eliminating confounders\n",
      "\n",
      "In dropping the -X -Y -Z variables (cartesian coordinates) we removed a large number of confounding variables as these have information strongly correlated with Magnitude + Angle (polar coordinates).   There may still be some confounding influences but the remaining effects are hard to interpret.  \n",
      "\n",
      "From common sense we see other variables -min, -max, -mad have correlations with mean/std so we drop all these confounders also.\n",
      "The number of variables is now reduced to 37 as below.  \n",
      "\n",
      "----\n",
      "\n",
      "Note to reviewers - we do some tedious name mapping to keep the semantics intact since we want to have a \"white box\" like model.\n",
      "If we don't we can just take the remaining variables and map them to v1, v2 ..... v37.  This would be a couple of lines of code and explanation but we would lose a lot of the value we derived from retaining interpretability using domain knowledge.  So we soldier on for just one last step and then we are into the happy land of analysis\n",
      "\n",
      "----"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Name transformations\n",
      "\n",
      "To be able to explore the data easily we rename variables and simplify them for readability as follows.\n",
      "\n",
      "We drop the \"Body\" and \"Mag\" wherever they occur as these are common to all our remaining variables.\n",
      "We map \u2018mean\u2019 to Mean and \u2018std\u2019 to SD\n",
      "\n",
      "So e.g.\n",
      "\n",
      "tAccBodyMag-mean -> tAccMean  \n",
      "fAccBodyMag-std -> fAccSD  \n",
      "etc.  "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Results\n",
      "\n",
      "The reduced set of selected variables with transformed names is now (with meaningful groupings):  \n",
      "\n",
      "* tAccMean, tAccSD tJerkMean, tJerkSD\n",
      "* tGyroMean, tGyroSD tGyroJerkMean, tGyroJerkSD\n",
      "* fAccMean, fAccSD, fJerkMean, fJerkSD,\n",
      "* fGyroMean, fGyroSD, fGyroJerkMean, fGyroJerkSD,\n",
      "* fGyroMeanFreq, fGyroJerkMeanFreq fAccMeanFreq, fJerkMeanFreq\n",
      "* fAccSkewness, fAccKurtosis, fJerkSkewness, fJerkKurtosis\n",
      "* fGyroSkewness, fGyroKurtosis fGyroJerkSkewness, fGyroJerkKurtosis\n",
      "* angleAccGravity, angleJerkGravity angleGyroGravity, angleGyroJerkGravity\n",
      "* angleXGravity, angleYGravity, angleZGravity\n",
      "* subject, activity  "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now after all these data cleanup calisthenics we raise our weary heads and notice something pleasantly surprising and positively encouraging.\n",
      "\n",
      "These variables are primarily magnitudes of acceleration and jerk with their statistics, along with angle variables.  This encourages us to think that our approach of focusing on domain knowledge, doing some extra reading and research and using some elementary physical intuition\n",
      "seems to be bearing fruit.  \n",
      "\n",
      "This is a set of variables that is semantically compact, interpretable and relatively easy to reason about.\n",
      "\n",
      "We can do another round of winnowing down the variables, because we might have a feeling that 37 variables is too many to hold in our mind at one time - and we would be right.  But at this point we bring in the heavy artillery and let the modeling software do the work, using Random Forests on this variable set."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Analysis.\n",
      "===\n",
      "\n",
      "\n",
      "Our goal for this phase is to use the reduced variable data set from our exploration phase to create a model predicting human activity, using Random Forests.\n",
      "\n",
      "To remind ourselves, the variables we will use are:\n",
      "\n",
      "* tAccMean, tAccSD tJerkMean, tJerkSD\n",
      "* tGyroMean, tGyroSD tGyroJerkMean, tGyroJerkSD\n",
      "* fAccMean, fAccSD, fJerkMean, fJerkSD,\n",
      "* fGyroMean, fGyroSD, fGyroJerkMean, fGyroJerkSD,\n",
      "* fGyroMeanFreq, fGyroJerkMeanFreq fAccMeanFreq, fJerkMeanFreq\n",
      "* fAccSkewness, fAccKurtosis, fJerkSkewness, fJerkKurtosis\n",
      "* fGyroSkewness, fGyroKurtosis fGyroJerkSkewness, fGyroJerkKurtosis\n",
      "* angleAccGravity, angleJerkGravity angleGyroGravity, angleGyroJerkGravity\n",
      "* angleXGravity, angleYGravity, angleZGravity\n",
      "* subject, activity  \n",
      "\n",
      "Of these,   \n",
      "\n",
      "* all except the last two are numeric.  \n",
      "* 'subject' is an integer identifying a person, one of 21 from 1 to 27 with some missing. \n",
      "* 'activity' is a categorical variable - one of six activities identified earlier -  \n",
      "* 'sitting', 'standing', 'lying', 'walking', 'walking up', 'walking down'.  \n",
      "\n",
      "Why do we use Random Forests? We are using Random Forests in our model due to the relatively high accuracy of this method and the complexity of our data.\n",
      "\n",
      "These are two major reasons to bring out the heavy artillery of Random Forests, especially when we have too many attrubutes even in a simplified set of attributes. \n",
      "\n",
      "## Methods\n",
      "\n",
      "\n",
      "### Expository Segue on Experiment design \n",
      "\n",
      "\n",
      "Typically in analysing such data sets we are creating a model that uses the data we are given.  How do we know the model will work for other data?  The real answer is \"We don't\".  And there's no way we can be sure that we can create a model that will work for new data.  \n",
      "\n",
      "But what we **can** do is reduce the chances that we are creating an \"overfitted\" model. That is a technical term for a model that works wonderfully on the given data (fitted to it) and fails on the next data set that comes along.  There's a way to design our modeling experiment so we avoid that trap.  Here's how.  \n",
      "\n",
      "We take the data set and we keep some of the given data aside and we don't use it for modeling at all.  This \"held out\" set is called the test set.\n",
      "\n",
      "Then we take our remaining data and we further divide it so that we have a larger set called the training set and a smaller set we call the validation set.\n",
      "Then we create our model using the training set and look at how well it performs on the validation set (i.e. not counting the \"held out\" data).  \n",
      "\n",
      "We are allowed to tweak our modeling as much as we want using the training and validations sets but we are **not** allowed to look at the held out, test set until we are ready to declare we are done modeling.  Then we apply the model to our held out test data -- when that test data also shows an acceptable error rate we have a good model.\n",
      "\n",
      "However if we get a bad error rate from the test data we have a problem.  We cannot keep tweaking the model to get a better test result because then we are simply overfitting again.  So what do we do?  We are allowed to mix up all the data, hold out a new test set which has to be different at least in part from the old one, and then we repeat the exercise.  In some cases when we are given a data set by a third party we are not shown the held out set, and we have to submit our model without testing agains the held out set.\n",
      "\n",
      "The third party then applies our model to the held out test set and we don't get to mix it all up.  We only get one shot.  We're going to do that here and see how well we do."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Our experiment design\n",
      "\n",
      "We hold out the last 4 subjects in the data as a test set and the rest are used for our modeling.   Why do we do this?  The data set, if we look at the supporting docs, suggests that we use the last 4 as test subjects.  So in our first pass at this we might as well just follow the instructions.  All rows relating to these 4 will be held out and not used during modeling at all.\n",
      "\n",
      "Of the 17 remaining subjects we use the first 12 subjects as the training set and remaining 5 as the validation set.   Why this proportion? Typically 30% of the the training data is used as validation set and 70% used for actual training.  The validation set is used as our \"internal\" test set, not used in modeling and held out for each validation step.  The difference between the actual test set and the validation set is that we are allowed to keep tuning our model as long as we keep mixing up the data after each attempt and re-extraction of a validation set.\n",
      "\n",
      "There is also a methodology that takes this step even further and does n-fold validation.  The training set is divided into n (usually 10) equal parts and then each part is used as a validation set while the rest used for training, with n such modeling exercises being conducted.\n",
      "Then some averaging is done to create the best model.  \n",
      "\n",
      "We do not do n-fold validation here.\n",
      "\n",
      "We divided our data based on the 'subject' variable as we included \u2018subject\u2019 in our model and want to keep all test data separate.  What does this mean?  The test data should actually be data about which we have no information at all - i.e. it needs to be independent of the training data.  So suppose we did not separate out the data on the 4 test individuals but we just decided that we would mix up all the rows and take say 20% as test data, chosen randomly.\n",
      "\n",
      "Note that we have some 7,000 plus rows so we have a few hundred rows on each individual.  So if we mixed it all up and chose randomly, then we would most probably get data from all 21 individuals in our test set.  And all 21 in our training set.  The test set would not be independent of the training set as both would have somewhat similar mixtures of data.\n",
      "Thus the held out set would not really provide a useful reality check - we have statistically similar info in our training set already i.e. the test set has leaked into the training set.\n",
      "\n",
      "This would be similar to the situation where we had a homework exercise which was later solved in class the next day. Then we received a quiz question set which had questions very similar to the homework with just some numbers changed.  It would not really test our understanding of the subject matter, only our ability to understand the homework (= overfitting).\n",
      "\n",
      "So when we keep aside our test set separated by all rows for certain individuals we know that the training set has no leaked information about these individuals.  It is important to be very diligent about the test data, in this fashion, so that we can have some confidence that our model is not overfitting our sample data.\ufffc\ufffc\ufffc\ufffc\ufffc\ufffc\ufffc\ufffc\ufffc\ufffc\ufffc\ufffc\ufffc\ufffc\ufffc\ufffc\ufffc\ufffc\ufffc"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Training\n",
      "\n",
      "We now run our RandomForest on our training set, described earlier, and derive a model along with some parameters describing how good our model is. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# pull in the training, validation and test sets created according to the scheme described\n",
      "# in the data exploration lesson.\n",
      "\n",
      "import pandas as pd\n",
      "\n",
      "samtrain = pd.read_csv('../data/samsung/samtrain.csv')\n",
      "samval = pd.read_csv('../data/samsung/samval.csv')\n",
      "samtest = pd.read_csv('../data/samsung/samtest.csv')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We use the Python RandomForest package from the scikits.learn collection of algorithms. \n",
      "\n",
      "The package is called sklearn.ensemble.RandomForestClassifier\n",
      "\n",
      "For this we need to convert the target column ('activity') to integer values because the Python RandomForest package requires that.\n",
      "\n",
      "### Exercise\n",
      "\n",
      "* map activity to an integer according to:\n",
      "    * laying = 1\n",
      "    * sitting = 2\n",
      "    * standing = 3\n",
      "    * walk = 4\n",
      "    * walkup = 5\n",
      "    * walkdown = 6"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Exercise: Train the Random Forest model on the samsung data"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Exercise: Inspect the Out of Band (OOB) score of the model.  \n",
      "\n",
      "# This represents its accuracy while training."
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Exercise: Look at the feature importances of the classifier.  \n",
      "\n",
      "# This is gives us some idea of which features influence the classification,\n",
      "# similar to inspecting the CPT tables of a Naive Bayes classifier."
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We use the predict() function using our model on our validation set and our test set and get the following results from our analysis of errors in the predictions."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Exercise: Make a Prediction with your now trained RandomForest classifier \n",
      "# (on both the validation and test sets)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "####Prediction Errors and Computed Error Measures  "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Exercise: Plot a confusion matrix to inspect the misclassification errors"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Exercise: Also print out the Precision, Recall, F1 score, and Accuracy of the model"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Results \n",
      "\n",
      "Focusing on magnitude and angle information for acceleration and jerk in the time and frequency domains gives us a model with surprising predictive power.  It's possible that a brute force model will give better predictive power but it would simply show us how to blindly apply software.  If for some reason the model misbehaved or failed, we would not have any insight at all as to why.  Instead we used domain knowledge to focus on insight and in the process created a model with interpretive value.\n",
      "\n",
      "Model performance on the test set is better than on the validation set as seen in the two \u201cTotal\u201d rows above and each individual activity.\n",
      "\n",
      "Let's see how we might be able to improve the model in future.  It's always good to note the possible ways in which our model(s) might be deficient or incomplete or unfinished so we don't get overconfident about our models and overpromise what they can do.\n",
      "\n",
      "### Critique\n",
      "\n",
      "* Our model eliminated a number of Magnitude related attributes such as -mad, -max, -min also a number of Gyro related variables during the variable selection process using domain knowledge. These may be important but this was not tested.  We may want to look at that the next time we do this.\n",
      "\n",
      "* Variable importance should be investigated in detail - i.e. we really ought to look at how we can use the smaller number of attributes identified as important, to create the model and see what the difference is. Computationally this would be more efficient. We could even use simpler methods like Logistic Regression to do the classification from that point on, using only the reduced set of variables."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Exercise\n",
      "\n",
      "Instead of using domain knowledge to reduce variables, use Random Forests directly on the full set of columns.  Then use variable importance and sort the variables.  \n",
      "\n",
      "Compare the model you get with the model you got from using domain knowledge. \n",
      "\n",
      "You can short circuit the data cleanup process as well by simply renaming the variables x1, x2...xn, y where y is 'activity' the dependent variable.  \n",
      "\n",
      "Now look at the new Random Forest model you get.  It is likely to be more accurate at prediction than the one we have above. It is a black box model, where there is no meaning attached to the variables.  \n",
      "          \n",
      "* What insights does it give you?  \n",
      "* Which model do you prefer?  \n",
      "* Why?  \n",
      "* Is this an absolute preference or might it change?  \n",
      "* What might cause it to change? "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from IPython.core.display import HTML\n",
      "def css_styling():\n",
      "    styles = open(\"../styles/custom.css\", \"r\").read()\n",
      "    return HTML(styles)\n",
      "css_styling()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<style>\n",
        "    @font-face {\n",
        "        font-family: \"Computer Modern\";\n",
        "        src: url('http://mirrors.ctan.org/fonts/cm-unicode/fonts/otf/cmunss.otf');\n",
        "    }\n",
        "    div.cell{\n",
        "        width:800px;\n",
        "        margin-left:auto;\n",
        "        margin-right:auto;\n",
        "    }\n",
        "    h1 {\n",
        "        font-family: \"Charis SIL\", Palatino, serif;\n",
        "    }\n",
        "    h4{\n",
        "        margin-top:12px;\n",
        "        margin-bottom: 3px;\n",
        "       }\n",
        "    div.text_cell_render{\n",
        "        font-family: Computer Modern, \"Helvetica Neue\", Arial, Helvetica, Geneva, sans-serif;\n",
        "        line-height: 145%;\n",
        "        font-size: 120%;\n",
        "        width:800px;\n",
        "        margin-left:auto;\n",
        "        margin-right:auto;\n",
        "    }\n",
        "    .CodeMirror{\n",
        "            font-family: \"Source Code Pro\", source-code-pro,Consolas, monospace;\n",
        "    }\n",
        "    .prompt{\n",
        "        display: None;\n",
        "    }\n",
        "    .text_cell_render h5 {\n",
        "        font-weight: 300;\n",
        "        font-size: 16pt;\n",
        "        color: #4057A1;\n",
        "        font-style: italic;\n",
        "        margin-bottom: .5em;\n",
        "        margin-top: 0.5em;\n",
        "        display: block;\n",
        "    }\n",
        "    \n",
        "    .warning{\n",
        "        color: rgb( 240, 20, 20 )\n",
        "        }\n",
        "</style>\n",
        "<script>\n",
        "    MathJax.Hub.Config({\n",
        "                        TeX: {\n",
        "                           extensions: [\"AMSmath.js\"]\n",
        "                           },\n",
        "                tex2jax: {\n",
        "                    inlineMath: [ ['$','$'], [\"\\\\(\",\"\\\\)\"] ],\n",
        "                    displayMath: [ ['$$','$$'], [\"\\\\[\",\"\\\\]\"] ]\n",
        "                },\n",
        "                displayAlign: 'center', // Change this to 'center' to center equations.\n",
        "                \"HTML-CSS\": {\n",
        "                    styles: {'.MathJax_Display': {\"margin\": 4}}\n",
        "                }\n",
        "        });\n",
        "</script>"
       ],
       "output_type": "pyout",
       "prompt_number": 1,
       "text": [
        "<IPython.core.display.HTML at 0x109b9bed0>"
       ]
      }
     ],
     "prompt_number": 1
    }
   ],
   "metadata": {}
  }
 ]
}